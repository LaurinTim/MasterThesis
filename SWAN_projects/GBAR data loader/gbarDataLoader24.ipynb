{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8306bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fnmatch, re\n",
    "import pytimber\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from time import mktime\n",
    "from tqdm import tqdm\n",
    "import lmfit\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "'''\n",
    "Replace the following four paths\n",
    "\n",
    "At DATAFILE the datafile24 is saved\n",
    "At DATASUMMARY a folder for the date will be created and the summary, short_summary and log are saved there\n",
    "At ELENASAVE the elena text file is saved.\n",
    "At INFOS the runData24 file is saved\n",
    "'''\n",
    "############################################################################################\n",
    "DATAFILE = Path('/eos') / 'user' / 'l' / 'lkoller' / 'GBAR' / 'data24'\n",
    "DATASUMMARY = Path('/eos') / 'user' / 'l' / 'lkoller' / 'GBAR' / 'data24' / 'datasummary24' \n",
    "ELENASAVE = Path('/eos') / 'user' / 'l' / 'lkoller' / 'GBAR' / 'data24' / 'elenadata24'\n",
    "INFOS = Path('/eos') / 'user' / 'l' / 'lkoller' / 'SWAN_projects' / 'data analysis 2024' / 'Data loader' / 'infos'\n",
    "############################################################################################\n",
    "\n",
    "PROTONGUNPC = Path('/eos') / 'experiment' / 'gbar' /'pgunpc' / 'data'\n",
    "RCPC = Path('/eos') / 'experiment' / 'gbar' / 'rcpc' / 'data'\n",
    "ELENADATA = Path('/eos') / 'experiment' / 'gbar' / 'elena' / 'data'\n",
    "RUNMODES = Path('/eos') / 'experiment' / 'gbar' /'pgunpc' / 'data' / 'Logs'\n",
    "\n",
    "#list with the names of the columns for the files in the pgungc/data/date/date.txt (replace date)\n",
    "sequence_logFile = ['Time', 'run_type', 'run', 'event_number', 'DAQ', 'pbar_trap', 'positron', 'Valve_in', 'MCP_in', 'MW_power', #8 general info\n",
    "                    'H-sou_pressure','cube_pressure','RC_pressure','MW_pressure','SwY_pressure','LyA_pressure', #14 pressures\n",
    "                   'Main_delay','PCOs_delay','LyA_delay','Laser_delay', #18 delays\n",
    "                   'FC_current', 'MCP4_current', 'MCP5_current', #21 currents\n",
    "                   'St_dec_up','St_dec_dw','St_dec_ri','St_dec_le','St_Cub_up','St_Cub_dw','St_Cub_ri','St_Cub_le', #29 dec and cub steerers\n",
    "                   'St_RC1_up','St_RC1_dw','St_RC1_ri','St_RC1_le','St_RC2_up','St_RC2_dw','St_RC2_ri','St_RC2_le','St_RC3_up','St_RC3_dw','St_RC3_ri','St_RC3_le', #41 RC1, RC2, RC3 steerers\n",
    "                   'St_Al1_up','St_Al1_dw','St_Al1_ri','St_Al1_le','St_Al2_up','St_Al2_dw','St_Al2_ri','St_Al2_le', #49 Al1, Al2 steerers\n",
    "                   'QT_RC1_+','QT_RC1_-','QT_RC2_+','QT_RC2_-','QT_RC3_+','QT_RC3_-','SwY_1_+','SwY_1_-','SwY_3_+','SwY_3_-', #59 qt and swy voltages\n",
    "                   'EL_pT_in','EL_pT_1','EL_pT_2','EL_pT_3','EL_pT_4','EL_RC_in','EL_SwY_+','EL_Al1_+','EL_Al2_+','EL_Al3_+', #69 EL voltages\n",
    "                   'Ly_MCP_1','Ly_MCP_2','Ly_MCP_3','Ly_MCP_4','Qnch_+','EL_Trg_-','TgDefl_+','H-Defl_+','H-Cor1_-','FrBias_+', #79 LyA MCPs bias, Quenching, TgDefl, H-Defl, H-Cor voltages\n",
    "                   '1_phos_+','1_mcp_+','2_phos_+','2_mcp_+','3_phos_+','3_mcp_+','3.5_grid_-','3.5_phos_+','3.5_mcp_+','4_phos_+','4_mcp_+','5_phos_+','5_mcp_+','6_phos_+','6_mcp_+','7_phos_+','7_mcp_+','Sci_1_-','Sci_2_-', #98  mcp + sci voltages\n",
    "                   'H_offs', 'target_pos', 'V_Led', #101 general info\n",
    "                   'mw_amp_curr', 'hfs_temp', 'hfs_freq', 'hfs_pow', 'sc_temp', 'sc_freq', 'sc_pow', #108 microwave parameters\n",
    "                   'empty_1', 'empty_2', 'empty_3', 'empty_4', 'empty_5', 'empty_6', 'empty_7', 'empty_8'] #116 empty columns\n",
    "\n",
    "#list with the names of the columns for the rcpc and pgunpc\n",
    "sequence_files = ['MCP1', 'MCP2', 'MCP3', 'MCP3.5', 'MCP4', 'MCP5', 'MCP7', 'Waveform_12bit', 'CMOS_Tracker', 'DRS4','Positron_Waveform',\n",
    "                'SD','LyA','SD_LyA']\n",
    "\n",
    "#list with the names of the columns for the data from elena\n",
    "sequence_elena = ['Cy_Des','NE00_I','NE50_I', 'NE00_Bm1', 'NE00_Bm2', 'NE00_Bm3', 'NE50_Bm1', 'NE50_Bm2', 'Int']\n",
    "\n",
    "##list with the order of the columns for the summary files\n",
    "sequence_summary = ['Time','Datetime','run_type','run','run_mode','event_number', 'DAQ', 'pbar_trap', 'positron', 'MCP1', 'MCP2', 'MCP3', 'MCP3.5', 'MCP4', 'MCP5', 'MCP7', 'Waveform_12bit', 'CMOS_Tracker', 'DRS4',\n",
    "                   'Positron_Waveform','SD','LyA','SD_LyA','Cy_Des','NE00_I','NE50_I', 'NE00_Bm1', 'NE00_Bm2', 'NE00_Bm3', 'NE50_Bm1', 'NE50_Bm2', 'Int',\n",
    "                   'Valve_in', 'MCP_in', 'MW_power', 'H-sou_pressure', 'cube_pressure', 'RC_pressure', 'MW_pressure','SwY_pressure','LyA_pressure', \n",
    "                   'Main_delay','PCOs_delay','LyA_delay','Laser_delay', \n",
    "                   'FC_current', 'MCP4_current', 'MCP5_current', \n",
    "                   'St_dec_up','St_dec_dw','St_dec_ri','St_dec_le','St_Cub_up','St_Cub_dw','St_Cub_ri','St_Cub_le', \n",
    "                   'St_RC1_up','St_RC1_dw','St_RC1_ri','St_RC1_le','St_RC2_up','St_RC2_dw','St_RC2_ri','St_RC2_le','St_RC3_up','St_RC3_dw','St_RC3_ri','St_RC3_le', \n",
    "                   'St_Al1_up','St_Al1_dw','St_Al1_ri','St_Al1_le','St_Al2_up','St_Al2_dw','St_Al2_ri','St_Al2_le', \n",
    "                   'QT_RC1_+','QT_RC1_-','QT_RC2_+','QT_RC2_-','QT_RC3_+','QT_RC3_-','SwY_1_+','SwY_1_-','SwY_3_+','SwY_3_-', \n",
    "                   'EL_pT_in','EL_pT_1','EL_pT_2','EL_pT_3','EL_pT_4','EL_RC_in','EL_SwY_+','EL_Al1_+','EL_Al2_+','EL_Al3_+', \n",
    "                   'Ly_MCP_1','Ly_MCP_2','Ly_MCP_3','Ly_MCP_4','Qnch_+','EL_Trg_-','TgDefl_+','H-Defl_+','H-Cor1_-','FrBias_+', \n",
    "                   '1_phos_+','1_mcp_+','2_phos_+','2_mcp_+','3_phos_+','3_mcp_+','3.5_grid_-','3.5_phos_+','3.5_mcp_+','4_phos_+','4_mcp_+','5_phos_+','5_mcp_+','6_phos_+','6_mcp_+','7_phos_+','7_mcp_+','Sci_1_-','Sci_2_-', \n",
    "                   'H_offs', 'target_pos', 'V_Led', \n",
    "                   'mw_amp_curr', 'hfs_temp', 'hfs_freq', 'hfs_pow', 'sc_temp', 'sc_freq', 'sc_pow', \n",
    "                   'empty_1', 'empty_2', 'empty_3', 'empty_4', 'empty_5', 'empty_6', 'empty_7', 'empty_8']\n",
    "    \n",
    "\n",
    "def loadVIS():\n",
    "    '''\n",
    "    \n",
    "    Set some general rules how dataframes and plots are displayed\n",
    "    \n",
    "    '''\n",
    "    pd.set_option(\"display.max_columns\",None)\n",
    "    plt.rcParams['axes.facecolor'] = 'white'\n",
    "    plt.rcParams['axes.spines.left'] = True #False\n",
    "    plt.rcParams['axes.spines.right'] = True #False\n",
    "    plt.rcParams['axes.spines.top'] = True #False\n",
    "    plt.rcParams['axes.spines.bottom'] = True #False\n",
    "    plt.rcParams['axes.grid'] = False\n",
    "    plt.rcParams['axes.grid.axis'] = 'both'\n",
    "    plt.rcParams['axes.labelcolor'] = '#555555'\n",
    "    plt.rcParams['text.color'] = 'black'\n",
    "    plt.rcParams['figure.figsize'] = 6,4\n",
    "    plt.rcParams['figure.dpi'] = 600\n",
    "    plt.rcParams['figure.titleweight'] = 'normal'\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "    \n",
    "def loadRun():\n",
    "    '''\n",
    "    \n",
    "    Put the run data starting from the 24_04_30 in a dataframe and save it at the path DATAFILE\n",
    "    From 24_07_23 11:04:02 onward the DAQlog file structure changed, the run type was added after the run number.\n",
    "    For runs before this, the run type is set to '00 Default'.\n",
    "    \n",
    "    '''\n",
    "    runa = list(csv.reader(list(open(RUNMODES / 'DAQlog.txt', 'r', encoding='windows-1252' )), delimiter = '\\t'))\n",
    "    runa = [val for val in runa if (len(val) > 2 and int(val[0][:10]) >= 1714463780)]\n",
    "    runc = []\n",
    "    \n",
    "    curr_pos = 0\n",
    "    while curr_pos < len(runa):\n",
    "        if 'Start_run: ' in runa[curr_pos] or 'Start: run/type: ' in runa[curr_pos]:\n",
    "            if float(runa[curr_pos][0]) < 1721725442.093:\n",
    "                runa[curr_pos][2] = 'Start: run/type: '\n",
    "                runa[curr_pos].insert(4, '00 Default')\n",
    "            runc += [runa[curr_pos]]\n",
    "        elif 'Stop-Evts: ' in runa[curr_pos] or 'Stop: evts/type: ' in runa[curr_pos]:\n",
    "            if float(runa[curr_pos][0]) < 1721725442.093:\n",
    "                runa[curr_pos][2] = 'Stop: evts/type: '\n",
    "                runa[curr_pos].insert(4, '00 Default')\n",
    "            runc[-1] += runa[curr_pos]\n",
    "        \n",
    "        curr_pos += 1\n",
    "    \n",
    "    runc = [val for val in runc if len(val) == 16]\n",
    "    \n",
    "    for i in ['Start: run/type: ', 'Set_name: ', 'Comment:  ', 'Stop: evts/type: ', 'Set_name: ']:\n",
    "        [val.remove(i) for val in runc]\n",
    "    \n",
    "    for i in range(len(runc)): del runc[i][9]\n",
    "    \n",
    "    runf = pd.DataFrame([val[:-1] for val in runc], columns = ['start run timestamp', 'start run datetime', 'run', 'run_type', 'setup', 'comment', 'stop run timestamp', 'stop run datetime', 'events'])\n",
    "    runf.to_csv(str(INFOS / ('runData24.txt')), sep='\\t', index=False)\n",
    "    \n",
    "    return pd.read_csv(str(INFOS) + '/runData24.txt', delimiter = '\\t')\n",
    "    \n",
    "\n",
    "def loadRunModes():\n",
    "    '''\n",
    "    Retrieve the content of RunModes_logs.txt as a dataframe. Contains the \"run mode\" (mixing / p background / Ps background / other )\n",
    "    \n",
    "    '''\n",
    "\n",
    "    empty = {'run':[],\n",
    "             'Run_code':[],\n",
    "             'Date_entered':[] }\n",
    "    rml = pd.DataFrame(empty)\n",
    "\n",
    "    if  (RUNMODES / 'RunModes_log.txt').is_file() == True :\n",
    "        rml=pd.read_csv(str(RUNMODES) + \"/RunModes_log.txt\",sep='\\t')\n",
    "\n",
    "    return rml\n",
    "\n",
    "\n",
    "def loadLog(date2an):\n",
    "    '''\n",
    "\n",
    "    Load the log file from the protongun PC and label the columns\n",
    "    The column at position 1 is split into two parts, the run type and run number.\n",
    "    Before 24_07_23 11:04:02 this column contains an integer with the run number and the run type gets set to 00.\n",
    "    After 24_07_23 11:04:02 there is a number of the form xx.yyyyy saved, where xx is the number associated with the run type and yyyyy is the run number.\n",
    "    The numbers for the different run types are:\n",
    "    00:  Default\n",
    "    01:  Mixing\n",
    "    02:  p-  BG\n",
    "    03:  e+ BG\n",
    "    04:  Ly alpha\n",
    "    05:  Pion Run\n",
    "    06:  Lp-N\n",
    "    07:  H- Test\n",
    "    08:  H- Detach\n",
    "    09:  p- devel\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    date2an = the date for which we want to create a log file\n",
    "    \n",
    "    '''\n",
    "    #if date2an is between 24_05_30 and 24_05_14 we use gbarDataLoader24_old which accounts for the different logfile structure\n",
    "    if '24_04_30' <= date2an <= '24_05_14':\n",
    "        log = go.loadLog(date2an)\n",
    "        return log\n",
    "    \n",
    "    #check if the log file from loadLog already exists for date2an\n",
    "    if ((DATASUMMARY / date2an / ('log_'+date2an+'.txt')).is_file() == True) and (date2an < datetime.datetime.fromtimestamp((DATASUMMARY / date2an / ('log_'+date2an+'.txt')).stat().st_mtime).strftime('%y_%m_%d')):\n",
    "        print('LOG file for '+date2an+' already exists.')\n",
    "        return pd.read_csv(str(DATASUMMARY / date2an / ('log_'+date2an+'.txt')), sep='\\t')\n",
    "    \n",
    "    else:\n",
    "        #check if the DAQ logfile exists for date2an\n",
    "        if not (PROTONGUNPC / date2an).exists() or not (PROTONGUNPC / date2an / (date2an+'.txt')).is_file():\n",
    "            return print('Wrong date format or date is not uploaded yet.')     \n",
    "        else:\n",
    "            #create folder for date2an at DATASUMMARY if there isnt one already\n",
    "            if not (DATASUMMARY/ date2an).exists():\n",
    "                Path.mkdir(DATASUMMARY / date2an)\n",
    "            \n",
    "            #path to save the logfile\n",
    "            logfile = PROTONGUNPC / date2an / (date2an+'.txt')\n",
    "            \n",
    "            #read the logfile from the DAQ                        \n",
    "            log = pd.read_csv(logfile, sep = '\\t', header = None)\n",
    "            \n",
    "            #set the values in the first column of log, in which the run type and run number are, to 0 and insert column for run type at position 1\n",
    "            log[1] = [0]*len(log)\n",
    "            log.insert(1, 0.5, [0]*len(log))\n",
    "            \n",
    "            #rename the columns of log to the ones in sequence_logFile\n",
    "            log.columns = np.arange(len(sequence_logFile))\n",
    "            for i in range(len(sequence_logFile)):\n",
    "                log = log.rename(columns={i: sequence_logFile[i]})\n",
    "            \n",
    "            #we want to set the run type and run number to the one corresponding to the times in loadRun()\n",
    "            runf = loadRun() #info about the runs\n",
    "                        \n",
    "            #we only need the part of runf that contains times in date2an\n",
    "            log_time = list(log.Time) #list with the times for each event in log\n",
    "            runf = runf[[True if (val <= log_time[0] <= bal or log_time[0] <= val <= bal <= log_time[-1] or val <= log_time[-1] <= bal) else False for val,bal in zip(runf['start run timestamp'],runf['stop run timestamp'])]].reset_index(drop = True)\n",
    "            runf_stime = list(runf['start run timestamp']) #list with start times of relevant runs\n",
    "            runf_ftime = list(runf['stop run timestamp']) #list with stop times of relevant runs\n",
    "            runf_run = list(runf.run) #list with run numbers of relevant runs\n",
    "            runf_type = [int(val[:2]) for val in runf.run_type] #list with numbers corresponding to run types of relevant runs\n",
    "            \n",
    "            for i in range(len(runf)):\n",
    "                log.loc[[True if runf_stime[i]<=val<=runf_ftime[i] else False for val in log_time], 'run'] = runf_run[i]\n",
    "                log.loc[[True if runf_stime[i]<=val<=runf_ftime[i] else False for val in log_time], 'run_type'] = runf_type[i]\n",
    "\n",
    "            log.to_csv(str(DATASUMMARY / date2an / ('log_'+date2an+'.txt')), sep='\\t', index=False)\n",
    "            return log\n",
    "        \n",
    "\n",
    "\n",
    "def loadRCPC(date2an):\n",
    "    '''\n",
    "    \n",
    "    Get the times and locations for the files from the different MCPs which are in the RCPC folder\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    date2an = the date for which we want to create a log file\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    dataframe with the times and filenames for the MCPSs at date2an sorted by time\n",
    "    \n",
    "    '''\n",
    "    mcp1_file = []\n",
    "    mcp1_time = []\n",
    "    mcp2_file = []\n",
    "    mcp2_time = []\n",
    "    mcp3_file = []\n",
    "    mcp3_time = []\n",
    "    mcp35_file = []\n",
    "    mcp35_time = []\n",
    "    mcp4_file = []\n",
    "    mcp4_time = []\n",
    "    mcp7_file = []\n",
    "    mcp7_time = []\n",
    "    \n",
    "    if (RCPC / date2an).exists() == False:\n",
    "        print('RCPC files might not have been uploaded yet.')\n",
    "    else:\n",
    "        p = RCPC / date2an\n",
    "        for file in p.iterdir():\n",
    "            if (fnmatch.fnmatch(file, '*MCP1*') or fnmatch.fnmatch(file, '*MCP-p1*')) and fnmatch.fnmatch(file, '*.tif') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp1_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp1_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*MCP2*') or fnmatch.fnmatch(file, '*MCP-p2*')) and fnmatch.fnmatch(file, '*.tif') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp2_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp2_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*MCP3*') or fnmatch.fnmatch(file, '*MCP-p3*')) and fnmatch.fnmatch(file, '*.tif') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp3_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp3_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*MCP3.5*') or fnmatch.fnmatch(file, '*MCP-p3.5*')) and fnmatch.fnmatch(file, '*.tif') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp35_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp35_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*PCO-ReC*') or fnmatch.fnmatch(file, '*MCP4*')) and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp4_time.append(int(re.search('s\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp4_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*VCXG-51M-7*') and fnmatch.fnmatch(file, '*.tif')) and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp7_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp7_file.append(file)\n",
    "        \n",
    "    df1 = pd.DataFrame({'Time' : mcp1_time, 'MCP1' : mcp1_file})\n",
    "    df2 = pd.DataFrame({'Time' : mcp2_time, 'MCP2' : mcp2_file})\n",
    "    df3 = pd.DataFrame({'Time' : mcp3_time, 'MCP3' : mcp3_file})\n",
    "    df35 = pd.DataFrame({'Time' : mcp35_time, 'MCP3.5' : mcp35_file})\n",
    "    df4 = pd.DataFrame({'Time' : mcp4_time, 'MCP4' : mcp4_file})\n",
    "    df7 = pd.DataFrame({'Time' : mcp7_time, 'MCP7' : mcp7_file})  \n",
    "    return pd.concat([df1,df2,df3,df35,df4,df7], ignore_index=True).sort_values('Time', ascending=True, ignore_index=True)\n",
    "        \n",
    "\n",
    "def loadPGUNPC(date2an):\n",
    "    '''\n",
    "    \n",
    "    files from the PGUNPC from date2an into a dataframe\n",
    "    \n",
    "    df5 are the locations of the pictures taken by MCP5\n",
    "    dfwf are the locations of the waveform files\n",
    "    dfcmos are the locations cmos detector\n",
    "    dfdrs is deinstalled now\n",
    "    dfposi1 are the locations positron waveform 1\n",
    "    dfposi2 are the locations positron waveform 2\n",
    "    dfposi3 are the locations positron waveform 3\n",
    "    dfposi4 are the locations positron waveform 4\n",
    "    dfsdposi are the locations png of the oscilloscope for the positrons\n",
    "    dflya are the locations of the trc files with the data of the 4 channels from the lyman alpha setup\n",
    "    dfsdlya are the locations png of the oscilloscope for the lya\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    dataframe with all the data from above sorted by time. The columns are:\n",
    "        df5 are the locations of the pictures taken by MCP5\n",
    "        dfwf are the locations of the waveform files\n",
    "        dfcmos are the locations cmos detector\n",
    "        dfdrs is deinstalled now\n",
    "        dfposi1 are the locations positron waveform 1\n",
    "        dfposi2 are the locations positron waveform 2\n",
    "        dfposi3 are the locations positron waveform 3\n",
    "        dfposi4 are the locations positron waveform 4\n",
    "        dfsdposi are the locations png of the oscilloscope for the positrons\n",
    "        dflya are the locations of the trc files with the data of the 4 channels from the lyman alpha setup\n",
    "        dfsdlya are the locations png of the oscilloscope for the lya    \n",
    "    \n",
    "    '''\n",
    "    mcp5_time, mcp5_file = [],[]\n",
    "    waveform_time, waveform_file = [],[]\n",
    "    cmos_time, cmos_file = [],[]\n",
    "    drs_time, drs_file = [],[]\n",
    "    posi_time, posi_file = [],[]\n",
    "    sd_posi_time, sd_posi_file = [],[]\n",
    "    lya_time, lya_file = [],[]\n",
    "    sd_lya_time, sd_lya_file = [],[]\n",
    "    \n",
    "    if (PROTONGUNPC / date2an).exists() == False:\n",
    "        print('PGunPC files might not have been uploaded yet.')\n",
    "    else:\n",
    "        p = PROTONGUNPC / date2an\n",
    "        posi = p / (date2an+'posi')\n",
    "        lya = p / (date2an+'lya')\n",
    "        for file in p.iterdir():\n",
    "            if (fnmatch.fnmatch(file, '*PCO-SwY*') or fnmatch.fnmatch(file, '*MCP6*')) and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                mcp5_time.append(int(re.search('s\\_(.*?)\\.', str(file)).group(1)))\n",
    "                mcp5_file.append(file)\n",
    "            if fnmatch.fnmatch(file, '*WF1234*') and fnmatch.fnmatch(file,'*.trc') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                waveform_time.append(int(re.search('WF1234\\.(.*?)\\.', str(file)).group(1)))\n",
    "                waveform_file.append(file)\n",
    "            if (fnmatch.fnmatch(file, '*VCXG-51M-5*') or fnmatch.fnmatch(file, '*BAU-TRK*')) and fnmatch.fnmatch(file, '*.tif') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                cmos_time.append(int(re.search('G16\\_(.*?)\\.', str(file)).group(1)))\n",
    "                cmos_file.append(file)\n",
    "            if fnmatch.fnmatch(file, '*drs4*') and fnmatch.fnmatch(file, '*.txt') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                filetime = re.search('drs4\\_(.*?)\\.', str(file)).group(1)\n",
    "                drs_time.append(int(mktime(datetime.datetime(int('20'+filetime[0:2]),int(filetime[3:5]),int(filetime[6:8]),int(filetime[9:11]), int(filetime[12:14]), int(filetime[15:17])).timetuple())))\n",
    "                drs_file.append(file)\n",
    "        if (posi.exists()):\n",
    "            for file in posi.iterdir():\n",
    "                if fnmatch.fnmatch(file, '*POS1234*') and fnmatch.fnmatch(file, '*.trc') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                    posi_time.append(int(re.search('POS1234\\.(.*?)\\.', str(file)).group(1)))\n",
    "                    posi_file.append(file)\n",
    "                if fnmatch.fnmatch(file, '*SD*') and fnmatch.fnmatch(file, '*.png') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                    sd_posi_time.append(int(re.search('SD\\.(.*?)\\.', str(file)).group(1)))\n",
    "                    sd_posi_file.append(file)\n",
    "        if (lya.exists()):\n",
    "            for file in lya.iterdir():\n",
    "                if fnmatch.fnmatch(file, '*LY*') and fnmatch.fnmatch(file, '*.trc') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                    try:\n",
    "                        lya_time.append(int(re.search('LY\\.(.*?)\\.', str(file)).group(1)))\n",
    "                    except:\n",
    "                        lya_time.append(int(re.search('LY1234\\.(.*?)\\.', str(file)).group(1)))\n",
    "                    lya_file.append(file)\n",
    "                if fnmatch.fnmatch(file, '*SD*') and fnmatch.fnmatch(file, '*.png') and fnmatch.fnmatch(file, '*.sys.v#.*') == False:\n",
    "                    sd_lya_time.append(int(re.search('SD\\.(.*?)\\.', str(file)).group(1)))\n",
    "                    sd_lya_file.append(file)\n",
    "    df5 = pd.DataFrame({'Time' : mcp5_time, 'MCP5' : mcp5_file})\n",
    "    dfwf = pd.DataFrame({'Time' : waveform_time, 'Waveform_12bit' : waveform_file})\n",
    "    dfcmos = pd.DataFrame({'Time' : cmos_time, 'CMOS_Tracker' : cmos_file})\n",
    "    dfdrs = pd.DataFrame({'Time' : drs_time, 'DRS4'  : drs_file})\n",
    "    dfposi = pd.DataFrame({'Time' : posi_time, 'Positron_Waveform' : posi_file})\n",
    "    dfsdposi = pd.DataFrame({'Time' : sd_posi_time, 'SD':sd_posi_file})\n",
    "    dflya = pd.DataFrame({'Time' : lya_time, 'LyA' : lya_file})\n",
    "    dfsdlya = pd.DataFrame({'Time' : sd_lya_time, 'SD_LyA':sd_lya_file})\n",
    "    return pd.concat([df5, dfwf, dfcmos, dfdrs, dfposi, dfsdposi, dflya, dfsdlya], ignore_index=True).sort_values('Time', ascending=True, ignore_index=True)\n",
    "\n",
    "def loadElena(date2an):\n",
    "    '''\n",
    "    \n",
    "    Get the parameters for the Elena/AD runs on the day date2an and return them in a dataframe\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    date2an = the date from which we want the data\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    elena = dataframe with the data for the elena runs on date2an, the colums are:\n",
    "        Time = unix timestamp of the cycle\n",
    "        Cy_Des = description of the cycle parameters\n",
    "        NE00_I = NE00 Intensity\n",
    "        NE50_I = NE50 Intensity\n",
    "        NE00_Bm1 = Whether or not the first beam monitor for the NE00 line is in\n",
    "        NE00_Bm2 = Whether or not the second beam monitor for the NE00 line is in\n",
    "        NE00_Bm3 = Whether or not the third beam monitor for the NE00 line is in\n",
    "        NE50_Bm1 = Whether or not the first beam monitor for the NE50 line is in\n",
    "        NE50_Bm2 = Whether or not the second beam monitor for the NE50 line is in\n",
    "        Int = Total intensity\n",
    "        Linac_Rad = Radiation monitor from the linac\n",
    "        \n",
    "    '''\n",
    "    if ((ELENASAVE / date2an / ('elena_' + date2an + '.txt')).is_file() == True):\n",
    "        #print('Elena file for ' + date2an + ' already exists.')\n",
    "        return pd.read_csv(str(ELENASAVE / date2an / ('elena_'+date2an+'.txt')), sep='\\t')\n",
    "    \n",
    "    if not (ELENADATA / date2an / ('ELENA_BM_' + date2an + '.csv')).is_file():\n",
    "        return print('The elena data for ' + date2an + ' has not been downloaded yet. Please run the ELENA_daily_data2text_file notebook.')\n",
    "    \n",
    "    if not (ELENASAVE/ date2an).exists():\n",
    "        Path.mkdir(ELENASAVE / date2an)\n",
    "    \n",
    "    bm = pd.read_csv(ELENADATA / date2an / ('ELENA_BM_' + date2an + '.csv'), header = 0, \n",
    "                     names = ['Time','NE00_I','NE50_I', 'NE00_Bm1', 'NE00_Bm2', 'NE00_Bm3', 'NE50_Bm1', 'NE50_Bm2', 'Int'])\n",
    "    cy = pd.read_csv(ELENADATA / date2an / ('ELENA_Cycles_' + date2an + '.csv'), header = 0, names = ['Time', 'Cy_Des'])\n",
    "    #lac = pd.read_csv(ELENADATA / date2an / ('Linac_RadMon_' + date2an + '.csv'), header = 0, names = ['Time', 'Linac_Rad'])\n",
    "    \n",
    "    if (bm.shape[0] != cy.shape[0]):\n",
    "        diff = len(bm) - len(cy)\n",
    "        if bm.iloc[0]['Time'] != cy.iloc[0]['Time']:\n",
    "            if diff > 0: bm = bm.iloc[diff:].reset_index(drop = True)\n",
    "            else: cy = cy.iloc[abs(diff):].reset_index(drop = True)\n",
    "        else:\n",
    "            if diff > 0: bm = bm.iloc[:-diff].reset_index(drop = True)\n",
    "            else: cy = cy.iloc[:diff].reset_index(drop = True)\n",
    "    \n",
    "    if bm.iloc[0]['Time'] != cy.iloc[0]['Time']:\n",
    "        print('ERROR: elena beam and cycle time do not match')\n",
    "    \n",
    "    bm = bm.drop(['Time'], axis = 1)\n",
    "    \n",
    "    elena = pd.concat([cy,bm], axis = 1).round({'Time':0}).sort_values(by = ['Time'])\n",
    "    \n",
    "    elena['Time'] = elena['Time'] + 12\n",
    "    \n",
    "    elena.to_csv(str(ELENASAVE / date2an / ('elena_'+date2an+'.txt')), sep='\\t', index=False)\n",
    "    \n",
    "    return elena\n",
    "\n",
    "\n",
    "def loadDatafile(date2an):\n",
    "    '''\n",
    "    \n",
    "    match all the data from elena, pgunpc and rcpc to the correct event in the log file\n",
    "    \n",
    "    '''\n",
    "    #if date2an is between 24_05_30 and 24_05_14 we use gbarDataLoader24_old which accounts for the different logfile structure\n",
    "    if '24_04_30' <= date2an <= '24_05_14':\n",
    "        df_final = go.loadDatafile(date2an)\n",
    "        return df_final\n",
    "    \n",
    "    if ((DATASUMMARY / date2an / ('summary_'+date2an+'.txt')).is_file() == True) and (date2an < datetime.datetime.fromtimestamp((DATASUMMARY / date2an / ('summary_'+date2an+'.txt')).stat().st_mtime).strftime('%y_%m_%d') ):\n",
    "        print('SUMMARY file for '+date2an+' already exists.')\n",
    "        return pd.read_csv(str(DATASUMMARY / date2an / ('summary_'+date2an+'.txt')), sep='\\t')\n",
    "    else:\n",
    "        if not (DATASUMMARY/ date2an).exists():\n",
    "            Path.mkdir(DATASUMMARY / date2an) \n",
    "        dfpgun = loadPGUNPC(date2an)\n",
    "        dfrcpc = loadRCPC(date2an)\n",
    "        dflog = loadLog(date2an)#.iloc[2088:]\n",
    "        \n",
    "        if type(dflog) != pd.core.frame.DataFrame and dflog == None:\n",
    "            return None\n",
    "        \n",
    "        strucdf = pd.DataFrame([[0]*len(sequence_summary)], columns = sequence_summary)\n",
    "\n",
    "        datafiles= pd.concat([dfpgun, dfrcpc], ignore_index=True).round({'Time':0}).sort_values(['Time']).reset_index(drop = True)\n",
    "        \n",
    "        empty_log = False\n",
    "        if len(datafiles) == 0:\n",
    "            datafiles.loc[0] = [0] * len(list(datafiles.columns))\n",
    "            empty_log = True\n",
    "            \n",
    "        #get the date of the previous day in the correct format\n",
    "        prev_date = [2000 + int(date2an[:2]), int(date2an[3:5]), int(date2an[-2:])]\n",
    "        prev_date = datetime.date(prev_date[0], prev_date[1], prev_date[2]) - datetime.timedelta(days=1)\n",
    "        prev_date = str(prev_date)[2:4] + '_' + str(prev_date)[5:7] + '_' + str(prev_date)[-2:]\n",
    "        \n",
    "        if (ELENASAVE / prev_date ).is_dir():\n",
    "            elena = pd.concat([loadElena(prev_date),loadElena(date2an)]).reset_index(drop = True)\n",
    "            \n",
    "        else:\n",
    "            elena = loadElena(date2an)\n",
    "            \n",
    "        for elabel in sequence_elena:\n",
    "            dflog.loc[0, elabel] = np.nan\n",
    "            \n",
    "        #return elena\n",
    "        \n",
    "        cont = True #track for datafiles if we can continue (for some reason break does not work?)\n",
    "        pos = 0\n",
    "        epos = 0\n",
    "        if len(dflog) > 0:\n",
    "            for idx, timestamp in tqdm(enumerate(dflog['Time']), total = len(dflog)):\n",
    "\n",
    "                for eidy, ets in enumerate(elena['Time'][epos:]):\n",
    "                    if (abs(timestamp - ets) < 7):\n",
    "                        epos = eidy + epos\n",
    "                        for elabel in sequence_elena:\n",
    "                            dflog.loc[idx, elabel] = elena[elabel][epos]\n",
    "                        break\n",
    "                \n",
    "                #if there was no data from ELENA found for the current event and the run type uses antiprotons, search for an antiproton event around 80 seconds before the time in the logfile\n",
    "                #!!! the nan thing is bad since eg mixing runs always use antiprotons, but I have to know which run types only use antiprotons !!!\n",
    "                if str(dflog.loc[idx, 'Cy_Des']) == 'nan' and dflog.loc[idx, 'run_type'] in [0,1,2,3,5,6,9]:\n",
    "                    temp_arr = [val for val in elena.Time if 77<timestamp-val<83]\n",
    "                    if len(temp_arr) == 1:\n",
    "                        curr_elena = elena[elena.Time == temp_arr[0]]\n",
    "                        if list(curr_elena.Cy_Des)[0] == 'Pbar_2024_NewOptics':\n",
    "                            for elabel in sequence_elena:\n",
    "                                dflog.loc[idx, elabel] = list(curr_elena[elabel])[0]\n",
    "                            epos = list(curr_elena.index)[0]\n",
    "\n",
    "                if not (timestamp - datafiles['Time'][0]) < -7:\n",
    "                    for idy, ts in enumerate(datafiles['Time'][pos:]):\n",
    "                        if abs(timestamp - ts) < 7:\n",
    "                            pos = idy + pos\n",
    "                            break\n",
    "                    while(cont == True and abs(datafiles['Time'][pos] - timestamp) < 7):\n",
    "                        for label in sequence_files:\n",
    "                            if (pd.isna(datafiles[label][pos]) == False): \n",
    "                                dflog.loc[idx, label] = datafiles[label][pos]\n",
    "                        pos += 1\n",
    "                        if (pos == len(datafiles)):\n",
    "                            cont = False\n",
    "        \n",
    "        dflog = pd.concat([dflog, pd.DataFrame({'Datetime' : [datetime.datetime.fromtimestamp(entry) for entry in dflog['Time']]})], axis=1)\n",
    "        runmodes = loadRunModes()[ [\"run\", \"RunType\"]]\n",
    "        runmodes = runmodes[[True if val in set(dflog.run) else False for val in runmodes.run]]\n",
    "        dflog = pd.merge(dflog, runmodes, how='outer', on=\"run\")\n",
    "        dflog = dflog.rename(columns = {'RunType': 'run_mode'})\n",
    "        \n",
    "        df_final = pd.DataFrame()\n",
    "        df_final = pd.concat([strucdf,dflog]).reset_index(drop = True).drop(0).reset_index(drop = True)\n",
    "        \n",
    "        #if there is no elena beam for an event in the logfile, we set all the elena parameters in df_final to 0 (except for Cy_Des which we set to 'None') so that if there is a file affiliated with the event, it does not get discarded in the short summary\n",
    "        df_final.loc[[True if str(val) == 'nan' else False for val in df_final['Cy_Des']], 'Cy_Des'] = 'None'\n",
    "        \n",
    "        for i in sequence_elena[1:]:\n",
    "            df_final.loc[[True if str(val) == 'nan' else False for val in df_final[i]], i] = 0\n",
    "        \n",
    "        df_final.to_csv(str(DATASUMMARY / date2an / ('summary_'+date2an+'.txt')), sep='\\t', index=False)\n",
    "        \n",
    "    return df_final\n",
    "\n",
    "\n",
    "def loadShortSummary(date2an):  \n",
    "    '''\n",
    "    \n",
    "    make a document 'shortSum_date2an.txt' which only contains the rows of summary_date2an.txt which have a non NaN value somewhere in rcpc or pgunpc\n",
    "    \n",
    "    '''\n",
    "    #if date2an is between 24_05_30 and 24_05_14 we use gbarDataLoader24_old which accounts for the different logfile structure\n",
    "    if '24_04_30' <= date2an <= '24_05_14':\n",
    "        shortData = go.loadShortSummary(date2an)\n",
    "        return shortData\n",
    "    \n",
    "    if ((DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')).is_file() == True):# and (date2an < datetime.fromtimestamp((DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')).stat().st_mtime).strftime('%y_%m_%d')):\n",
    "        print('SHORT SUMMARY file for '+date2an+' already exists.')\n",
    "        return pd.read_csv(str(DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')), sep='\\t')\n",
    "    else: \n",
    "        longData = loadDatafile(date2an)\n",
    "        elena = loadElena(date2an)\n",
    "        log = loadLog(date2an)\n",
    "        \n",
    "        if (type(log) != pd.core.frame.DataFrame and log == None) or (type(longData) != pd.core.frame.DataFrame and longData == None):\n",
    "            return None\n",
    "\n",
    "        #drop all rows that have only nan for the pgunpc and rcpc (the threshold len(sequence_logFile) + len(sequence_elena) + 3 should be correct, not 100% sure)\n",
    "        shortData = longData.dropna(thresh=len(sequence_logFile) + len(sequence_elena) + 2).reset_index(drop = True)\n",
    "        \n",
    "        #we put all the cells with nan to str(nan) since we do not want mixed types in the textfiles\n",
    "        for i in sequence_files:\n",
    "            shortData.loc[[True if str(val) == 'nan' else False for val in shortData[i]], i] = 'None'\n",
    "        \n",
    "        shortData.to_csv(str(DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')), sep='\\t', index = False)\n",
    "\n",
    "    return shortData\n",
    "\n",
    "\n",
    "def addDate(date2an):\n",
    "    '''\n",
    "    \n",
    "    add the short summary from date2an to datafile24.txt\n",
    "    if no datafile24.txt exists, we create a new one\n",
    "    \n",
    "    '''\n",
    "    #if date2an is between 24_05_30 and 24_05_14 we use gbarDataLoader24_old which accounts for the different logfile structure\n",
    "    if '24_04_30' <= date2an <= '24_05_14':\n",
    "        go.addDate(date2an)\n",
    "        return\n",
    "    \n",
    "    newfile = False #track we we created a new datafile24\n",
    "    \n",
    "    #if datafile24.txt does not exist, create a new one\n",
    "    if (DATAFILE / 'datafile24.txt').is_file() == False:\n",
    "        newfile = True\n",
    "        datafile = pd.DataFrame([['00_00_00'] + [1]*len(sequence_summary)], columns = ['Date'] + sequence_summary)\n",
    "\n",
    "    else:\n",
    "        datafile = pd.read_csv(str(DATAFILE) + '/datafile24.txt', delimiter = '\\t') #get the datafile\n",
    "\n",
    "    #check that date2an isnt already in the file\n",
    "    for i in datafile['Date']:\n",
    "        if i == date2an:\n",
    "            return print('The date ' + date2an + ' has already been added to the datafile.')\n",
    "        \n",
    "    summary = loadShortSummary(date2an) #load the summary for date2an, which we want to add to the file \n",
    "    if type(summary) != pd.core.frame.DataFrame and summary == None:\n",
    "        return None\n",
    "    summary.insert(0, 'Date', [date2an] * len(summary))\n",
    "    \n",
    "    datafile = pd.concat([datafile, summary]).sort_values(by = ['Time'], axis = 0).reset_index(drop = True) #add the summary to the datafile and sort by the date\n",
    "    \n",
    "    if (newfile == True):\n",
    "        datafile = datafile.drop([0])\n",
    "        \n",
    "    datafile.to_csv(str(DATAFILE) + '/datafile24.txt', sep='\\t', index = False)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def replaceDate(date2an):\n",
    "    '''\n",
    "    \n",
    "    same as addDate, but if date2an is already in the datafile it replaces it, as well as log, summary and short summary\n",
    "    \n",
    "    '''\n",
    "    removeDate(date2an)\n",
    "    \n",
    "    if (DATASUMMARY / date2an / ('log_'+date2an+'.txt')).is_file():\n",
    "        os.remove((DATASUMMARY / date2an / ('log_'+date2an+'.txt')))\n",
    "    if (DATASUMMARY / date2an / ('summary_'+date2an+'.txt')).is_file():\n",
    "        os.remove((DATASUMMARY / date2an / ('summary_'+date2an+'.txt')))\n",
    "    if (DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')).is_file():\n",
    "        os.remove((DATASUMMARY / date2an / ('shortSum_'+date2an+'.txt')))\n",
    "      \n",
    "    addDate(date2an)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def removeDate(date2an):\n",
    "    '''\n",
    "    \n",
    "    remove the data from date2an from the datafile24.txt\n",
    "    \n",
    "    '''\n",
    "    #if date2an is between 24_05_30 and 24_05_14 we use gbarDataLoader24_old which accounts for the different logfile structure\n",
    "    if '24_04_30' <= date2an <= '24_05_14':\n",
    "        go.removeDate(date2an)\n",
    "        return\n",
    "    \n",
    "    #if datafile24.txt does not exist, we just stop\n",
    "    if (DATAFILE / 'datafile24.txt').is_file() == False:\n",
    "        return print('The datafile does not exist yet.')\n",
    "    \n",
    "    #get the datafile\n",
    "    datafile = pd.read_csv(str(DATAFILE) + '/datafile24.txt', delimiter = '\\t', index_col = [0])\n",
    "    \n",
    "    #check if the date is in the datafile\n",
    "    date_exists = False \n",
    "    for i in datafile.index:\n",
    "        if i == date2an:\n",
    "            date_exists = True\n",
    "            break\n",
    "    \n",
    "    #if the date is not in the datafile, we can stop\n",
    "    if (date_exists == False):\n",
    "        return print('The date ' + date2an + ' is not in the datafile.')\n",
    "    \n",
    "    #remove the date from the datafiles\n",
    "    datafile = datafile.drop(date2an, axis = 0)\n",
    "    \n",
    "    datafile.to_csv(str(DATAFILE) + '/datafile24.txt', sep='\\t') #save the datafile with the removed date\n",
    "    \n",
    "    return print('The date ' + date2an + ' has been removed from the datafile.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1daa960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
